{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24df71395f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(x_real, x_fake):\n",
    "    ''' For visualizing images '''\n",
    "    image_tensor = torch.cat((x_fake[:1, ...], x_real[:1, ...]), dim=0)\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat, nrow=1)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, mode='train'):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
    "        if len(self.files_A) > len(self.files_B):\n",
    "            self.files_A, self.files_B = self.files_B, self.files_A\n",
    "        self.new_perm()\n",
    "        assert len(self.files_A) > 0, \"Make sure you downloaded the horse2zebra images!\"\n",
    "\n",
    "    def new_perm(self):\n",
    "        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n",
    "        if item_A.shape[0] != 3:\n",
    "            item_A = item_A.repeat(3, 1, 1)\n",
    "        if item_B.shape[0] != 3:\n",
    "            item_B = item_B.repeat(3, 1, 1)\n",
    "        if index == len(self) - 1:\n",
    "            self.new_perm()\n",
    "        return item_A, item_B\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.listdir(\".\")) < 3:\n",
    "    !wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
    "    !unzip horse2zebra.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    '''\n",
    "    AdaptiveInstanceNorm2d Class\n",
    "    Values:\n",
    "        channels: the number of channels the image has, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, s_dim=8, h_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
    "        self.style_scale_transform = self.mlp(s_dim, h_dim, channels)\n",
    "        self.style_shift_transform = self.mlp(s_dim, h_dim, channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def mlp(self, in_dim, h_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(h_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, image, w):\n",
    "        '''\n",
    "        Function for completing a forward pass of AdaIN: Given an image and a style,\n",
    "        returns the normalized image that has been scaled and shifted by the style.\n",
    "        Parameters:\n",
    "          image: the feature map of shape (n_samples, channels, width, height)\n",
    "          w: the intermediate noise vector w to be made into the style (y)\n",
    "        '''\n",
    "        normalized_image = self.instance_norm(image)\n",
    "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
    "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
    "        transformed_image = style_scale * normalized_image + style_shift\n",
    "        return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    '''\n",
    "    LayerNorm2d Class\n",
    "    Values:\n",
    "        channels: number of channels in input, a scalar\n",
    "        affine: whether to apply affine denormalization, a bool\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.rand(channels))\n",
    "            self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.flatten(1).mean(1).reshape(-1, 1, 1, 1)\n",
    "        std = x.flatten(1).std(1).reshape(-1, 1, 1, 1)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            x = x * self.gamma.reshape(1, -1, 1, 1) + self.beta.reshape(1, -1, 1, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualBlock Class\n",
    "    Values:\n",
    "        channels: number of channels throughout residual block, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, s_dim=None, h_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3)\n",
    "            ),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3)\n",
    "            ),\n",
    "        )\n",
    "        self.use_style = s_dim is not None and h_dim is not None\n",
    "        if self.use_style:\n",
    "            self.norm1 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
    "            self.norm2 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
    "        else:\n",
    "            self.norm1 = nn.InstanceNorm2d(channels)\n",
    "            self.norm2 = nn.InstanceNorm2d(channels)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        x_id = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x, s) if self.use_style else self.norm1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x, s) if self.use_style else self.norm2(x)\n",
    "        return x + x_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentEncoder(nn.Module):\n",
    "    '''\n",
    "    ContentEncoder Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_downsample: number of downsampling layers, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=64, n_downsample=2, n_res_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = base_channels\n",
    "\n",
    "        # Input convolutional layer\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(3, channels, kernel_size=7)\n",
    "            ),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Downsampling layers\n",
    "        for i in range(n_downsample):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.InstanceNorm2d(2 * channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "\n",
    "        # Residual blocks\n",
    "        layers += [\n",
    "            ResidualBlock(channels) for _ in range(n_res_blocks)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.out_channels = channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @property\n",
    "    def channels(self):\n",
    "        return self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEncoder(nn.Module):\n",
    "    '''\n",
    "    StyleEncoder Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_downsample: number of downsampling layers, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "    '''\n",
    "\n",
    "    n_deepen_layers = 2\n",
    "\n",
    "    def __init__(self, base_channels=64, n_downsample=4, s_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = base_channels\n",
    "\n",
    "        # Input convolutional layer\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(3, channels, kernel_size=7, padding=0)\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Downsampling layers\n",
    "        for i in range(self.n_deepen_layers):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "        for i in range(n_downsample - self.n_deepen_layers):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "\n",
    "        # Apply global pooling and pointwise convolution to style_channels\n",
    "        layers += [\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, s_dim, kernel_size=1),\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder Class\n",
    "    Values:\n",
    "        in_channels: number of channels from encoder output, a scalar\n",
    "        n_upsample: number of upsampling layers, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, n_upsample=2, n_res_blocks=4, s_dim=8, h_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = in_channels\n",
    "\n",
    "        # Residual blocks with AdaIN\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(channels, s_dim) for _ in range(n_res_blocks)\n",
    "        ])\n",
    "\n",
    "        # Upsampling blocks\n",
    "        layers = []\n",
    "        for i in range(n_upsample):\n",
    "            layers += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.ReflectionPad2d(2),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, channels // 2, kernel_size=5)\n",
    "                ),\n",
    "                LayerNorm2d(channels // 2),\n",
    "            ]\n",
    "            channels //= 2\n",
    "\n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, 3, kernel_size=7)\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x, s=s)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_downsample: number of downsampling layers, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channels: int = 64,\n",
    "        n_c_downsample: int = 2,\n",
    "        n_s_downsample: int = 4,\n",
    "        n_res_blocks: int = 4,\n",
    "        s_dim: int = 8,\n",
    "        h_dim: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.c_enc = ContentEncoder(\n",
    "            base_channels=base_channels, n_downsample=n_c_downsample, n_res_blocks=n_res_blocks,\n",
    "        )\n",
    "        self.s_enc = StyleEncoder(\n",
    "            base_channels=base_channels, n_downsample=n_s_downsample, s_dim=s_dim,\n",
    "        )\n",
    "        self.dec = Decoder(\n",
    "            self.c_enc.channels, n_upsample=n_c_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        content = self.c_enc(x)\n",
    "        style = self.s_enc(x)\n",
    "        return (content, style)\n",
    "\n",
    "    def decode(self, content, style):\n",
    "        return self.dec(content, style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_layers: number of downsampling layers, a scalar\n",
    "        n_discriminators: number of discriminators (all at different scales), a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channels: int = 64,\n",
    "        n_layers: int = 3,\n",
    "        n_discriminators: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            self.patchgan_discriminator(base_channels, n_layers) for _ in range(n_discriminators)\n",
    "        ])\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def patchgan_discriminator(base_channels, n_layers):\n",
    "        '''\n",
    "        Function that constructs and returns one PatchGAN discriminator module.\n",
    "        '''\n",
    "        channels = base_channels\n",
    "        # Input convolutional layer\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(3, channels, kernel_size=4, stride=2),\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Hidden convolutional layers\n",
    "        for _ in range(n_layers):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "\n",
    "        # Output projection layer\n",
    "        layers += [\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, 1, kernel_size=1)\n",
    "            ),\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for discriminator in self.discriminators:\n",
    "            outputs.append(discriminator(x))\n",
    "            x = self.downsample(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GinormousCompositeLoss(nn.Module):\n",
    "    '''\n",
    "    GinormousCompositeLoss Class: implements all losses for MUNIT\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def image_recon_loss(x, gen):\n",
    "        c, s = gen.encode(x)\n",
    "        recon = gen.decode(c, s)\n",
    "        return F.l1_loss(recon, x), c, s\n",
    "\n",
    "    @staticmethod\n",
    "    def latent_recon_loss(c, s, gen):\n",
    "        x_fake = gen.decode(c, s)\n",
    "        recon = gen.encode(x_fake)\n",
    "        return F.l1_loss(recon[0], c), F.l1_loss(recon[1], s), x_fake\n",
    "\n",
    "    @staticmethod\n",
    "    def adversarial_loss(x, dis, is_real):\n",
    "        preds = dis(x)\n",
    "        target = torch.ones_like if is_real else torch.zeros_like\n",
    "        loss = 0.0\n",
    "        for pred in preds:\n",
    "            loss += F.mse_loss(pred, target(pred))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUNIT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gen_channels: int = 64,\n",
    "        n_c_downsample: int = 2,\n",
    "        n_s_downsample: int = 4,\n",
    "        n_res_blocks: int = 4,\n",
    "        s_dim: int = 8,\n",
    "        h_dim: int = 256,\n",
    "        dis_channels: int = 64,\n",
    "        n_layers: int = 3,\n",
    "        n_discriminators: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen_a = Generator(\n",
    "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
    "        )\n",
    "        self.gen_b = Generator(\n",
    "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
    "        )\n",
    "        self.dis_a = Discriminator(\n",
    "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
    "        )\n",
    "        self.dis_b = Discriminator(\n",
    "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
    "        )\n",
    "        self.s_dim = s_dim\n",
    "        self.loss = GinormousCompositeLoss\n",
    "\n",
    "    def forward(self, x_a, x_b):\n",
    "        s_a = torch.randn(x_a.size(0), self.s_dim, 1, 1, device=x_a.device).to(x_a.dtype)\n",
    "        s_b = torch.randn(x_b.size(0), self.s_dim, 1, 1, device=x_b.device).to(x_b.dtype)\n",
    "\n",
    "        # Encode real x and compute image reconstruction loss\n",
    "        x_a_loss, c_a, s_a_fake = self.loss.image_recon_loss(x_a, self.gen_a)\n",
    "        x_b_loss, c_b, s_b_fake = self.loss.image_recon_loss(x_b, self.gen_b)\n",
    "\n",
    "        # Decode real (c, s) and compute latent reconstruction loss\n",
    "        c_b_loss, s_a_loss, x_ba = self.loss.latent_recon_loss(c_b, s_a, self.gen_a)\n",
    "        c_a_loss, s_b_loss, x_ab = self.loss.latent_recon_loss(c_a, s_b, self.gen_b)\n",
    "\n",
    "        # Compute adversarial losses\n",
    "        gen_a_adv_loss = self.loss.adversarial_loss(x_ba, self.dis_a, True)\n",
    "        gen_b_adv_loss = self.loss.adversarial_loss(x_ab, self.dis_b, True)\n",
    "\n",
    "        # Sum up losses for gen\n",
    "        gen_loss = (\n",
    "            10 * x_a_loss + c_b_loss + s_a_loss + gen_a_adv_loss + \\\n",
    "            10 * x_b_loss + c_a_loss + s_b_loss + gen_b_adv_loss\n",
    "        )\n",
    "\n",
    "        # Sum up losses for dis\n",
    "        dis_loss = (\n",
    "            self.loss.adversarial_loss(x_ba.detach(), self.dis_a, False) + \\\n",
    "            self.loss.adversarial_loss(x_a.detach(), self.dis_a, True) + \\\n",
    "            self.loss.adversarial_loss(x_ab.detach(), self.dis_b, False) + \\\n",
    "            self.loss.adversarial_loss(x_b.detach(), self.dis_b, True)\n",
    "        )\n",
    "\n",
    "        return gen_loss, dis_loss, x_ab, x_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
    "\n",
    "munit_config = {\n",
    "    'gen_channels': 64,\n",
    "    'n_c_downsample': 2,\n",
    "    'n_s_downsample': 4,\n",
    "    'n_res_blocks': 4,\n",
    "    's_dim': 8,\n",
    "    'h_dim': 256,\n",
    "    'dis_channels': 64,\n",
    "    'n_layers': 3,\n",
    "    'n_discriminators': 3,\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "munit = MUNIT(**munit_config).to(device).apply(weights_init)\n",
    "\n",
    "# Initialize dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(286),\n",
    "    transforms.RandomCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset('horse2zebra', transform),\n",
    "    batch_size=1, pin_memory=True, shuffle=True,\n",
    ")\n",
    "\n",
    "# Initialize optimizers\n",
    "gen_params = list(munit.gen_a.parameters()) + list(munit.gen_b.parameters())\n",
    "dis_params = list(munit.dis_a.parameters()) + list(munit.dis_b.parameters())\n",
    "gen_optimizer = torch.optim.Adam(gen_params, lr=1e-4, betas=(0.5, 0.999))\n",
    "dis_optimizer = torch.optim.Adam(dis_params, lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse torch version for autocast\n",
    "# ######################################################\n",
    "version = torch.__version__\n",
    "version = tuple(int(n) for n in version.split('.')[:-1])\n",
    "has_autocast = version >= (1, 6)\n",
    "# ######################################################\n",
    "\n",
    "def train(munit, dataloader, optimizers, device):\n",
    "\n",
    "    max_iters = 1000000\n",
    "    decay_every = 100000\n",
    "    cur_iter = 0\n",
    "\n",
    "    display_every = 500\n",
    "    mean_losses = [0., 0.]\n",
    "\n",
    "    while cur_iter < max_iters:\n",
    "        for (x_a, x_b) in tqdm(dataloader):\n",
    "            x_a = x_a.to(device)\n",
    "            x_b = x_b.to(device)\n",
    "\n",
    "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
    "            # If you're running older versions of torch, comment this out\n",
    "            # and use NVIDIA apex for mixed/half precision training\n",
    "            if has_autocast:\n",
    "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                    outputs = munit(x_a, x_b)\n",
    "            else:\n",
    "                outputs = munit(x_a, x_b)\n",
    "\n",
    "            losses, x_ab, x_ba = outputs[:-2], outputs[-2], outputs[-1]\n",
    "            munit.zero_grad()\n",
    "\n",
    "            for i, (optimizer, loss) in enumerate(zip(optimizers, losses)):\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                mean_losses[i] += loss.item() / display_every\n",
    "\n",
    "            cur_iter += 1\n",
    "\n",
    "            if cur_iter % display_every == 0:\n",
    "                print('Step {}: [G loss: {:.5f}][D loss: {:.5f}]'\n",
    "                      .format(cur_iter, *mean_losses))\n",
    "                show_tensor_images(x_ab, x_a)\n",
    "                show_tensor_images(x_ba, x_b)\n",
    "                mean_losses = [0., 0.]\n",
    "\n",
    "            if cur_iter == max_iters:\n",
    "                break\n",
    "\n",
    "            # Schedule learning rate by 0.5\n",
    "            if cur_iter % decay_every == 0:\n",
    "                for optimizer in optimizers:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] *= 0.5\n",
    "\n",
    "train(\n",
    "    munit, dataloader,\n",
    "    [gen_optimizer, dis_optimizer],\n",
    "    device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
