{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def batch_generator(inputs, batch_size):\n",
    "    \"\"\"\n",
    "    Generates batches of `batch_size` from `inputs` array.\n",
    "    \"\"\"\n",
    "    l = inputs.shape[0]\n",
    "    for i in range(0, l, batch_size):\n",
    "        yield inputs[i:min(i + batch_size, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('tiny_nerf_data.npz'):\n",
    "    !wget https://bmild.github.io/nerf/tiny_nerf_data.npz\n",
    "\n",
    "data = np.load('tiny_nerf_data.npz')\n",
    "images = data['images']\n",
    "poses = data['poses']\n",
    "focal = data['focal']\n",
    "print(images.shape, poses.shape, focal)\n",
    "\n",
    "testimg, testpose = images[101], poses[101]\n",
    "# use the first 100 images for training\n",
    "images = images[:100,...,:3]\n",
    "poses = poses[:100]\n",
    "\n",
    "plt.imshow(testimg)\n",
    "plt.show()\n",
    "\n",
    "images = torch.from_numpy(images).to(device)\n",
    "poses = torch.from_numpy(poses).to(device)\n",
    "testimg = torch.from_numpy(testimg).to(device)\n",
    "testpose = torch.from_numpy(testpose).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(height, width, focal_length, cam2world):\n",
    "    \"\"\"\n",
    "    Compute the rays (origins and directions) passing through an image with\n",
    "    `height` and `width` (in pixels). `focal_length` (in pixels) is a property\n",
    "    of the camera. `cam2world` represents and transform tensor from a 3D point\n",
    "    in the \"camera\" frame of reference to the \"world\" frame of reference (the\n",
    "    `pose` in our dataset).\n",
    "    \"\"\"\n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(width).to(cam2world),\n",
    "        torch.arange(height).to(cam2world),\n",
    "        indexing=\"xy\"\n",
    "    )\n",
    "    dirs = torch.stack([\n",
    "        (i.cpu() - width / 2) / focal_length,\n",
    "        - (j.cpu() - height / 2) / focal_length,\n",
    "        - torch.ones_like(i.cpu())\n",
    "    ], dim=-1).to(cam2world)\n",
    "    rays_d = torch.sum(dirs[..., None, :] * cam2world[:3, :3], dim=-1)\n",
    "    rays_o = cam2world[:3, -1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(x, L_embed=6):\n",
    "    \"\"\"\n",
    "    Returns tensor representing positional encoding $\\gamma(x)$ of `x` with\n",
    "    `L_embed` corresponding to $L$ in the above.\n",
    "    \"\"\"\n",
    "    rets = [x]\n",
    "    for i in range(L_embed):\n",
    "        for fn in [torch.sin, torch.cos]:\n",
    "            rets.append(fn(2 ** i * x))\n",
    "    return torch.cat(rets, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNeRF(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements 4 layer MLP as a tiny example of the NeRF design\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim=128, L_embed=6):\n",
    "        super().__init__()\n",
    "        in_dim = 3 + 3 * 2 * L_embed\n",
    "        self.layer1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim + in_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.layer1(x))\n",
    "        out = F.relu(self.layer2(out))\n",
    "        out = F.relu(self.layer3(torch.cat([out, x], dim=-1)))\n",
    "        out = self.layer4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(\n",
    "    model, rays_o, rays_d, near, far, N_samples, encoding_fn, rand=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Use `model` to render the rays parameterized by `rays_o` and `rays_d`\n",
    "    between `near` and `far` limits with `N_samples`.\n",
    "    \"\"\"\n",
    "    # sample query pts\n",
    "    z_vals = torch.linspace(near, far, N_samples).to(rays_o)\n",
    "    if rand:\n",
    "        z_vals = (\n",
    "            torch.rand(list(rays_o.shape[:-1]) + [N_samples])\n",
    "            * (far - near) / N_samples\n",
    "        ).to(rays_o) + z_vals\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
    "\n",
    "    # run query pts through model to get radiance fields\n",
    "    pts_flat = pts.reshape((-1, 3))\n",
    "    encoded_pts_flat = encoding_fn(pts_flat)\n",
    "    batches = batch_generator(encoded_pts_flat, batch_size=BATCH_SIZE)\n",
    "    preds = []\n",
    "    for batch in batches:\n",
    "        preds.append(model(batch))\n",
    "    radiance_fields_flat = torch.cat(preds, dim=0)\n",
    "    radiance_fields = torch.reshape(\n",
    "        radiance_fields_flat, list(pts.shape[:-1]) + [4]\n",
    "    )\n",
    "\n",
    "    # compute densities and colors\n",
    "    sigma_a = F.relu(radiance_fields[..., 3])\n",
    "    rgb = torch.sigmoid(radiance_fields[..., :3])\n",
    "\n",
    "    # do volume rendering\n",
    "    oneE10 = torch.tensor([1e10], dtype=rays_o.dtype, device=rays_o.device)\n",
    "    dists = torch.cat([\n",
    "        z_vals[..., 1:] - z_vals[..., :-1],\n",
    "        oneE10.expand(z_vals[..., :1].shape)\n",
    "    ], dim=-1)\n",
    "    alpha = 1 - torch.exp(-sigma_a * dists)\n",
    "    weights = torch.roll(torch.cumprod(1 - alpha + 1e-10, dim=-1), 1, dims=-1)\n",
    "    weights[..., 0] = 1\n",
    "    weights = alpha * weights\n",
    "\n",
    "    rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n",
    "    depth_map = (weights * z_vals).sum(dim=-1)\n",
    "    acc_map = weights.sum(dim=-1)\n",
    "    return rgb_map, depth_map, acc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "NUM_ENCODING_FUNCTIONS = 6\n",
    "NEAR = 2\n",
    "FAR = 6\n",
    "DEPTH_SAMPLES = 64\n",
    "LEARNING_RATE = 5e-3\n",
    "BATCH_SIZE = 16384\n",
    "NUM_EPOCHS = 1000\n",
    "DISPLAY_EVERY = 100\n",
    "HEIGHT, WIDTH = images.shape[1:3]\n",
    "FOCAL = data['focal']\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# initialize encoding function, model, loss, and optimizer\n",
    "encoding_fn = lambda x: positional_encoding(x, L_embed=NUM_ENCODING_FUNCTIONS)\n",
    "model = TinyNeRF(L_embed=NUM_ENCODING_FUNCTIONS)\n",
    "model.to(device)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# for plotting the loss and iteration during training\n",
    "psnrs = []\n",
    "iternums = []\n",
    "\n",
    "for i in range(NUM_EPOCHS + 1):\n",
    "    # sample an image from our training set\n",
    "    img_idx = np.random.randint(images.shape[0])\n",
    "    target = images[img_idx].to(device)\n",
    "    pose = poses[img_idx].to(device)\n",
    "\n",
    "    # get the rays passing through the image and forward pass the model\n",
    "    rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, pose)\n",
    "    rgb, _, _ = render_rays(\n",
    "        model, rays_o, rays_d, near=NEAR, far=FAR, N_samples=DEPTH_SAMPLES,\n",
    "        encoding_fn=encoding_fn\n",
    "    )\n",
    "\n",
    "    # backward pass\n",
    "    loss = loss_fn(rgb, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # plot the model's render of the test image and loss at each iteration\n",
    "    if i % DISPLAY_EVERY == 0:\n",
    "        rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, testpose)\n",
    "        rgb, _, _ = render_rays(\n",
    "            model, rays_o, rays_d, near=NEAR, far=FAR, N_samples=DEPTH_SAMPLES,\n",
    "            encoding_fn=encoding_fn\n",
    "        )\n",
    "        loss = loss_fn(rgb, testimg)\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        psnr = -10 * torch.log10(loss)\n",
    "        psnrs.append(psnr.item())\n",
    "        iternums.append(i)\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(rgb.detach().cpu().numpy())\n",
    "        plt.title(f'Iteration: {i}')\n",
    "        plt.subplot(122)\n",
    "        plt.plot(iternums, psnrs)\n",
    "        plt.title('PSNR')\n",
    "        plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some transformation tensors for translations and rotations about\n",
    "# different axes\n",
    "trans_t = lambda t : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "rot_phi = lambda phi : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,np.cos(phi),-np.sin(phi),0],\n",
    "    [0,np.sin(phi), np.cos(phi),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "rot_theta = lambda th : torch.tensor([\n",
    "    [np.cos(th),0,-np.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [np.sin(th),0, np.cos(th),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    \"\"\"\n",
    "    Compute a transformation tensor for a spherical coordinates\n",
    "    (`theta`, `phi`, `radius`)\n",
    "    \"\"\"\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w.numpy()\n",
    "    return c2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run poses that encircle the object through our trained model and make a video\n",
    "frames = []\n",
    "for th in np.linspace(0., 360., 120, endpoint=False):\n",
    "    c2w = pose_spherical(th, -30, 4)\n",
    "    c2w = torch.from_numpy(c2w).to(device).float()\n",
    "    rays_o, rays_d = get_rays(HEIGHT, WIDTH, FOCAL, c2w[:3,:4])\n",
    "    rgb, _, _ = render_rays(\n",
    "        model, rays_o, rays_d, NEAR, FAR, N_samples=DEPTH_SAMPLES,\n",
    "        encoding_fn=encoding_fn\n",
    "    )\n",
    "    frames.append((255*np.clip(rgb.cpu().detach().numpy(),0,1)).astype(np.uint8))\n",
    "\n",
    "import imageio\n",
    "f = 'video.mp4'\n",
    "imageio.mimwrite(f, frames, fps=30, quality=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the video in the notebook\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('video.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls autoplay loop>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
